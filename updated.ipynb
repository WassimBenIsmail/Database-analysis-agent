<artifact identifier="fixed-notebook" type="application/vnd.ant.code" language="python" title="Fixed Router & Tool Evaluations Notebook">
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbWoGOis4KoG"
   },
   "source": [
    "# Router & Tool Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will implement the following evaluators to assess the performance of the router and the tools:\n",
    "- an LLM-as-a-judge to evaluate the correctness of the router's function calling choice and the correctness of the parameters extracted;\n",
    "- an LLM-as-a-judge to evaluate the correctness of the SQL generated by tool 1 and the clarity of the analysis generated by tool 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from phoenix.evals import (\n",
    "    TOOL_CALLING_PROMPT_TEMPLATE, \n",
    "    llm_classify,\n",
    "    OpenAIModel\n",
    ")\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "from openinference.instrumentation import suppress_tracing\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"evaluating-agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_agent, start_main_span, tools, get_phoenix_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Agent with a Set of Testing Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\"\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    try:\n",
    "        ret = start_main_span([{\"role\": \"user\", \"content\": question}])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Evals using LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOOL_CALLING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Required Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    \"span_kind == 'LLM'\",\n",
    ").select(\n",
    "    question=\"input.value\",\n",
    "    tool_call=\"llm.tools\"\n",
    ")\n",
    "\n",
    "tool_calls_df = px.Client().query_spans(query, \n",
    "                                        project_name=PROJECT_NAME, \n",
    "                                        timeout=None)\n",
    "tool_calls_df = tool_calls_df.dropna(subset=[\"tool_call\"])\n",
    "\n",
    "tool_calls_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    tool_call_eval = llm_classify(\n",
    "        dataframe = tool_calls_df,\n",
    "        template = TOOL_CALLING_PROMPT_TEMPLATE.template[0].template.replace(\"{tool_definitions}\", \n",
    "                                                                 json.dumps(tools).replace(\"{\", '\"').replace(\"}\", '\"')),\n",
    "        rails = ['correct', 'incorrect'],\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "tool_call_eval['score'] = tool_call_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "\n",
    "tool_call_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Analysis Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLARITY_LLM_JUDGE_PROMPT = \"\"\"\n",
    "In this task, you will be presented with a query and an answer. Your objective is to evaluate the clarity \n",
    "of the answer in addressing the query. A clear response is one that is precise, coherent, and directly \n",
    "addresses the query without introducing unnecessary complexity or ambiguity. An unclear response is one \n",
    "that is vague, disorganized, or difficult to understand, even if it may be factually correct.\n",
    "\n",
    "Your response should be a single word: either \"clear\" or \"unclear,\" and it should not include any other \n",
    "text or characters. \"clear\" indicates that the answer is well-structured, easy to understand, and \n",
    "appropriately addresses the query. \"unclear\" indicates that some part of the response could be better \n",
    "structured or worded.\n",
    "\n",
    "[BEGIN DATA]\n",
    "Query: {query}\n",
    "Answer: {response}\n",
    "[END DATA]\n",
    "\n",
    "EXPLANATION: Provide your reasoning step by step, evaluating the clarity of the answer based on the query.\n",
    "LABEL: \"clear\" or \"unclear\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    \"span_kind=='AGENT'\"\n",
    ").select(\n",
    "    response=\"output.value\",\n",
    "    query=\"input.value\"\n",
    ")\n",
    "\n",
    "clarity_df = px.Client().query_spans(query, \n",
    "                                     project_name=PROJECT_NAME,\n",
    "                                     timeout=None)\n",
    "\n",
    "clarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    clarity_eval = llm_classify(\n",
    "        dataframe = clarity_df,\n",
    "        template = CLARITY_LLM_JUDGE_PROMPT,\n",
    "        rails = ['clear', 'unclear'],\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "clarity_eval['score'] = clarity_eval.apply(lambda x: 1 if x['label']=='clear' else 0, axis=1)\n",
    "\n",
    "clarity_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Response Clarity\", dataframe=clarity_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating SQL Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_EVAL_GEN_PROMPT = \"\"\"\n",
    "SQL Evaluation Prompt:\n",
    "-----------------------\n",
    "You are tasked with determining if the SQL generated appropriately answers a given instruction.\n",
    "\n",
    "Data:\n",
    "-----\n",
    "- [Instruction]: {question}\n",
    "- [Reference Query]: {query_gen}\n",
    "\n",
    "Evaluation:\n",
    "-----------\n",
    "Your response should be a single word: either \"correct\" or \"incorrect\".\n",
    "\n",
    "- \"correct\" indicates that the sql query correctly solves the instruction.\n",
    "- \"incorrect\" indicates that the sql query does not solve the instruction correctly.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    \"span_kind=='LLM'\"\n",
    ").select(\n",
    "    query_gen=\"llm.output_messages\",\n",
    "    question=\"input.value\",\n",
    ")\n",
    "\n",
    "sql_df = px.Client().query_spans(query, \n",
    "                                 project_name=PROJECT_NAME,\n",
    "                                 timeout=None)\n",
    "sql_df = sql_df[sql_df[\"question\"].str.contains(\"Generate an SQL query based on a prompt.\", na=False)]\n",
    "\n",
    "sql_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    sql_gen_eval = llm_classify(\n",
    "        dataframe = sql_df,\n",
    "        template = SQL_EVAL_GEN_PROMPT,\n",
    "        rails = ['correct', 'incorrect'],\n",
    "        model=OpenAIModel(model=\"gpt-4o\"),\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "sql_gen_eval['score'] = sql_gen_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "\n",
    "sql_gen_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"SQL Gen Eval\", dataframe=sql_gen_eval),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
</artifact>